P8105 HW3
================
Elaine Yanxi Chen
2022-10-16

## Packages and settings

First we load the packages necessary to knit this document.

``` r
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1: `instacart`

## Import data

``` r
data("instacart")
```

## Describe the dataset

-   This dataset contains 1384617 rows and 15 columns, with each row
    representing a single product from an instacart order.

-   Variables include identifiers for user, order, and product; the
    order in which each product was added to the cart. There are several
    order-level variables, describing the day and time of the order, and
    number of days since prior order. Then there are several
    item-specific variables, describing the product name (e.g. Yogurt,
    Avocado), department (e.g. dairy and eggs, produce), and aisle
    (e.g. yogurt, fresh fruits), and whether the item has been ordered
    by this user in the past.

-   In total, there are 39123 products found in 131209 orders from
    131209 distinct users.

## Answer questions

-   How many aisles are there, and which aisles are the most items
    ordered from?

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

There are 134 aisles, and the most items are ordered from the fresh
vegetables and fresh fruits aisles.

-   Make a plot that shows the number of items ordered in each aisle,
    limiting this to aisles with more than 10000 items ordered. Arrange
    aisles sensibly, and organize your plot so others can read it.

``` r
instacart %>% 
   count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

<img src="p8105_hw3_yc4017_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

-   Make a table showing the three most popular items in each of the
    aisles “baking ingredients”, “dog food care”, and “packaged
    vegetables fruits”. Include the number of times each item is ordered
    in your table.

``` r
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  knitr::kable()
```

| aisle                      | product_name                                    |    n | rank |
|:---------------------------|:------------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                            | 3324 |    1 |
| packaged vegetables fruits | Organic Raspberries                             | 1920 |    2 |
| packaged vegetables fruits | Organic Blueberries                             | 1692 |    3 |
| baking ingredients         | Light Brown Sugar                               |  157 |    1 |
| baking ingredients         | Pure Baking Soda                                |  140 |    2 |
| baking ingredients         | Organic Vanilla Extract                         |  122 |    3 |
| dog food care              | Organix Grain Free Chicken & Vegetable Dog Food |   14 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe             |   13 |    2 |
| dog food care              | Original Dry Dog                                |    9 |    3 |

-   Make a table showing the mean hour of the day at which Pink Lady
    Apples and Coffee Ice Cream are ordered on each day of the week;
    format this table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>% 
  filter(product_id == c(3798, 17334)) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the
    ## `.groups` argument.

| product_name     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.22 | 15.00 | 15.33 | 15.40 | 15.17 | 10.33 | 12.35 |
| Pink Lady Apples | 12.25 | 11.68 | 12.00 | 13.94 | 11.91 | 13.87 | 11.56 |

Note that there are two names: Pink Lady Apples and Pink Lady (Cripps)
Apples. Not sure if they are the same product, but they do have
different `product_id`, so will use that to filter (both give the same
tibble).

# Problem 2: Accelerometer

## Import and tidy data

``` r
accel_data = read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names()
```

    ## Rows: 35 Columns: 1443
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
accel_df = accel_data %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "min_of_day",
    values_to = "activity_count",
    names_prefix = "activity_"
  ) %>% 
  rename(dow = day) %>% 
  mutate(
    dow = as.factor(dow),
    dow = fct_relevel(dow, "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
    weekend = case_when(
    dow %in% c("Saturday", "Sunday") ~ "weekend",
    dow != c("Saturday", "Sunday") ~ "weekday",
    TRUE ~""
  ),
  min_of_day = as.numeric(min_of_day))
```

-   Load, tidy, and otherwise wrangle the data.

    -   Dataset was transformed using `pivot_longer` so that each row
        represents the activity count for a single minute in a 24-hour
        day.

-   Include all originally observed variables and values.

-   Include a weekday vs weekend variable.

-   Encode data with reasonable variable classes.

    -   `day` and `weekend` are coded as characters, while the rest of
        the variables are coded as numeric.

## Describe the data

Describe the resulting dataset (e.g. what variables exist, how many
observations, etc).

-   This dataset contains 50400 rows and 6 columns, with each row
    representing the activity count for a single minute in a 24-hour day
    for the 63-year-old male.

-   Variables include identifiers for the day, the minute, and the
    corresponding activity count. There are several day-level variables,
    describing which day (`day_id`)or which week (`week`) the activity
    count is from, out of the five week data collection period. It also
    describes which day of the week the measurement was from (`dow`) and
    whether it’s a weekday or a weekend (`weekend`). And then there is
    the minute-specific variable (`min_of_day`) that specifies the
    minute of the activity count measurement, starting at midnight of a
    24-hour day. Finally, we have the activity count (`activit_count`)
    measured by the accelerometers for each specific minute.

-   In total, there are activity counts from 1440 minutes each day for a
    total of 35 days spanning 5 weeks.

## Total activity over the day

We want to aggregate across minutes to create a total activity variable
for each day and show results in a table.

``` r
accel_df %>% group_by(week, dow) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = dow,
    values_from = total_activity
  ) %>% 
  select("week", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday") %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'week'. You can override using the
    ## `.groups` argument.

| week |    Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday | Sunday |
|-----:|----------:|---------:|----------:|---------:|---------:|---------:|-------:|
|    1 |  78828.07 | 307094.2 |    340115 | 355923.6 | 480542.6 |   376254 | 631105 |
|    2 | 295431.00 | 423245.0 |    440962 | 474048.0 | 568839.0 |   607175 | 422018 |
|    3 | 685910.00 | 381507.0 |    468869 | 371230.0 | 467420.0 |   382928 | 467052 |
|    4 | 409450.00 | 319568.0 |    434460 | 340291.0 | 154049.0 |     1440 | 260617 |
|    5 | 389080.00 | 367824.0 |    445366 | 549658.0 | 620860.0 |     1440 | 138421 |

Looking at the table, it seems that the total activity are higher for
weekdays when compared to the weekends. However, we see that there are
two extreme outliers on the Saturday of weeks 4 and 5. Interestingly,
both observations have a total activity count of 1440, which corresponds
to the number of minutes in a day. It is possible that these were
erroneous measurements.

To look at trends, we can generate some quick plots by piping the
results into `ggplot2`.

Here, I want to look at three trends:

-   Does the total activity change as the weeks go on?

-   How does the activity level during the week compares with the
    weekends?

-   Is there any notably trend of activity level associated with the day
    of the week?

``` r
num_of_week = accel_df %>% group_by(week, day_id, dow, weekend) %>% summarize(total_activity = sum(activity_count)) %>% 
  ggplot(aes(x = day_id, y = total_activity)) +
  geom_point() + geom_line()
```

    ## `summarise()` has grouped output by 'week', 'day_id', 'dow'. You can override
    ## using the `.groups` argument.

``` r
weekend_plot = accel_df %>% group_by(week, day_id, dow, weekend) %>% summarize(total_activity = sum(activity_count)) %>% 
  ggplot(aes(x = weekend, y = total_activity)) +
  geom_boxplot()
```

    ## `summarise()` has grouped output by 'week', 'day_id', 'dow'. You can override
    ## using the `.groups` argument.

``` r
dow_plot = accel_df %>% group_by(week, day_id, dow, weekend) %>% summarize(total_activity = sum(activity_count)) %>% 
  ggplot(aes(x = dow, y = total_activity)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5, hjust = 1))
```

    ## `summarise()` has grouped output by 'week', 'day_id', 'dow'. You can override
    ## using the `.groups` argument.

``` r
num_of_week / weekend_plot / dow_plot
```

<img src="p8105_hw3_yc4017_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

Looking at the quick plots, we can see that:

-   It does not seem like there is any obvious trend in total activity
    as the weeks go on. One can argue there may be a slight decrease of
    total activity near the end of five-week period.

-   On average, it seems that weekdays have higher total activity when
    compared to the weekend.

-   Looking across the week, we see that Wednesdays, Fridays, and
    arguably, Sundays, have higher total activity count. Comparing to
    weekdays, the total activity for the weekends have larger range.

-   All three plots are affected by the extreme outliers stated earlier.

## 24-hour activity time courses

Make a single-panel plot that shows the 24-hour activity time courses
for each day and use color to indicate day of the week.

``` r
accel_df %>% 
  group_by(week, day_id) %>% 
  ggplot(aes(x = min_of_day, y = activity_count, colour = dow)) +
  geom_line() +
  labs(
    title = "24-hour Activity Time Course Plot",
    x = "Hour of the day (hr)",
    y = "Activity count",
    colour = "Day of the week"
  ) +
   scale_x_continuous(
    breaks = c(0, 120, 240, 360, 480, 600, 720, 840, 960, 1080, 1200, 1320, 1440),
    labels = c("0", "2", "4", "6", "8", "10", "12", "14", "16", "18", "20", "22", "24")) +
  scale_y_continuous(
    breaks = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000)
  )
```

<img src="p8105_hw3_yc4017_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

Minutes on the x-axis were converted to hours of the day to help better
read and interpret the plot. The activity count is generally low (around
1000) from hours 0 - 6 in the morning, which corresponds to the time of
sleep. The activity count picks up from around 6 am, and is much higher
during the day till around hour 23 or 11 pm at night. There are several
notable activity count peaks. There are several peaks around 7 - 8 am,
10 am - 12 pm, around 4 pm, and most notably, around 8 - 10 pm.

# Problem 3: NY NOAA data

## Import data

``` r
data("ny_noaa")

ny_noaa = ny_noaa %>% 
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin))

summary(ny_noaa)
```

    ##       id                 date                 prcp               snow       
    ##  Length:2595176     Min.   :1981-01-01   Min.   :    0.00   Min.   :  -13   
    ##  Class :character   1st Qu.:1988-11-29   1st Qu.:    0.00   1st Qu.:    0   
    ##  Mode  :character   Median :1997-01-21   Median :    0.00   Median :    0   
    ##                     Mean   :1997-01-01   Mean   :   29.82   Mean   :    5   
    ##                     3rd Qu.:2005-09-01   3rd Qu.:   23.00   3rd Qu.:    0   
    ##                     Max.   :2010-12-31   Max.   :22860.00   Max.   :10160   
    ##                                          NA's   :145838     NA's   :381221  
    ##       snwd             tmax              tmin        
    ##  Min.   :   0.0   Min.   :-389.0    Min.   :-594.0   
    ##  1st Qu.:   0.0   1st Qu.:  50.0    1st Qu.: -39.0   
    ##  Median :   0.0   Median : 150.0    Median :  33.0   
    ##  Mean   :  37.3   Mean   : 139.8    Mean   :  30.3   
    ##  3rd Qu.:   0.0   3rd Qu.: 233.0    3rd Qu.: 111.0   
    ##  Max.   :9195.0   Max.   : 600.0    Max.   : 600.0   
    ##  NA's   :591786   NA's   :1134358   NA's   :1134420

## Exploration of the dataset

-   This dataset contains 2595176 rows and 7 columns, with each row
    representing the weather parameters recorded from a weather station
    in the state of New York on the specific date.

-   Variables include identifiers for the weather station (`id`), the
    date of observation (`date`), and the corresponding weather
    observations.

-   There are several observation-level variables, describing the amount
    of precipitation (`prcp`), the amount of snowfall and snow depth
    (`snow` and `snwd`), and the minimum and maximum temperature of the
    day (`tmin` and `tmax`).

-   In total, there are weather observations recorded from 747 weather
    stations in the state of New York from January 1st, 1981 to December
    31st, 2010.

-   Because each weather station may collect only a subset of these
    variables, the resulting dataset contains extensive missing data.
    For example, 43.71% of minimum temperature data is missing. This
    could have significant impact on the data analysis. Simply removing
    all missing data could easily introduce bias.

## Data cleaning

``` r
weather_df = ny_noaa %>% 
 separate(col = date, into = c("year", "month", "day"), sep = "-", convert = TRUE) 

weather_df = weather_df %>% 
  mutate(prcp = prcp / 10,
         tmax = tmax / 10,
         tmin = tmin / 10,
         month = month.abb[as.numeric(month)])
```

Because `prcp`, `tmax` and `tmin` were recorded in tenth of the desired
units (mm or degrees C), we first convert them to the reasonable units
of mm or degrees C. For snowfall, the most commonly observed values is
0. This is because for most of the year there is no or very minimal
snowfall in the state of New York, and record will only have non-zero
values in late fall and winter seasons when snowfall actually occurs.

## Two-panel plot for average max temp

``` r
tmax_avg = weather_df %>% 
  filter(month == c("Jan", "Jul")) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, colour = id)) + 
  geom_line(alpha = .5) + 
  facet_grid(~ month) +
  theme(legend.position = "none") +
    labs(
      title = "Average maximum temperature in January and in July in New York from 1981 to 2010",
    x = "Year",
    y = "Maximum average temperature (C)") +
   scale_x_continuous(
    breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
  scale_y_continuous(
    breaks = c(-15, -10, -5, 0, 5, 10, 15, 20, 25, 30, 35, 40)
  ) + 
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5, hjust = 1))
```

    ## `summarise()` has grouped output by 'id', 'year'. You can override using the
    ## `.groups` argument.

``` r
tmax_avg
```

<img src="p8105_hw3_yc4017_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

The average maximum temperature in January ranges around -10 C to 10 C,
while in July it ranges around 22 C to 32 C. It is interesting to note
that the average maximum temperature for both January and July seem to
oscillate up and down around a centred value of 0 C for January and
roughly 27 C for July. One can also argue that there is a very slight
increase of maximum average temperature for both months from 1980 to
2010: the highest and the lowest maximum average temperature seem to be
getting more extreme, possibly due to climate change. There are several
outliers in this plot, for example, January 2014 and July 1987.

## Two-panel for `tmax` vs `tmin` and snowfall distribution

We now want a two-panel showing

-   `tmax` vs. `tmin` for the full dataset

-   a plot showing the distribution of snowfall values greater than 0
    and less than 100 separetely by year

``` r
tmin_tmax = weather_df %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  theme(legend.position = "right") +
    labs(
      title = "Maximum and Minimum Temperature of 
      New York State in Years 1981 to 2010",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)") +
   scale_x_continuous(
    breaks = c(-60, -40, -20, 0, 20, 40, 60)) +
  scale_y_continuous(
    breaks = c(-60, -40, -20, 0, 20, 40, 60)
  ) + 
  theme(legend.position = "right")

snowfall_dist = weather_df %>% 
  filter(snow == c(1:99)) %>% 
  group_by(year) %>% 
  ggplot(aes(x = snow, y = as.factor(year))) + geom_density_ridges(scale = 0.85) +
  labs(
      title = "Snowfall Values (mm) in New York 
      from 1981 to 2010",
    x = "Snowfall (mm)",
    y = "Year") +
   scale_x_continuous(
    breaks = c(0, 20, 40, 60, 80, 100, 120)) 

tmin_tmax + snowfall_dist
```

    ## Picking joint bandwidth of 8.97

<img src="p8105_hw3_yc4017_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />
